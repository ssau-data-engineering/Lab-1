# Лабораторная работа №1
### Выполнил студент 6231-010402D Непряхин Богдан Олегович

## Подключение к Docker

- Скопирован Репозиторицй с GIT Prerequisites 
- Используя compose файлы развернуты airflow, nifi и Kibana-Elasticsearch для накопления и визуализации данных

* Перед тем как начать работу рекомендуется установить докер и wsl на тот диск где будет минимум 20+- свободных GB иначе места может не хватать для выплнения работы

## Работа с **Apache Airflow**

- В качествен данных - будем использовать набор из нескольких CSV файлов, полученных из набора данных [wine-review](https://www.kaggle.com/datasets/zynicide/wine-reviews/)
CSV файлы с набором данных [wine-review](https://www.kaggle.com/datasets/zynicide/wine-reviews/) в папке`data` переместили в папку с airflow, чтобы был коннект с исходными данными.
- Написали DAG с тасками для выполнения процедуры ETL [air_csv](./result_airflow/air_csv.py)
- Запущен DAG и началось выполнение загрузки данных со сбором единого файла
![airflow](./images/airflow.png)

- После того как файл собран, данные из него построчно отправлены в Elasticsearch в json формате
- Данные доступны в БД
![Elasticsearch](./images/Kibana_airflow.png)

- Проиндексировали все значения и создали запись в Index manager
![Elastic_data](./images/Elastic_data.png)

- В Kibana визуализирволали полученные данные
![Elastic_data](./images/Pie_Visual_Elastic.png)


## Работа с **Apache Nifi**

Предварительно тот же датасет со всеми данным перемещаем в папку с Nifi `nifi/data/lab_1/input` и запускаем Nifi  <http://localhost:18080/nifi>
- Создаем первый процессор и добавляем функция GetFile
![GetFile](./images/nifi_getfile.png) прописываем настройки в Properties и задаем каталог где расположены файлы 

- Добавляем новый процессор со SplitRecord и перетаскивая стрелочку на него от GetFile чтобы добавить связь. В настройках подключения выбираем succes и далее в splite добавляем настойки для него уже
![SplitRecord](./images/nifi_split.png)

* После добавления процессора SplitRecord в нем важно указать ReadCSV и WriteCSV коннектор. Для этого переходим в control_service 
![control_service](./images/control_service1.png)
И активируем эти контроллеры
![control_service](./images/control_service2.png)


- Далее создаем процессор со QueryRecord куда указываем данные для забота данных в формате SQL. Его соединяем со SplitRecord и передаем в связи значения полученные в split. Прописываем настройки для QueryRecord^
![QueryRecord](./images/nifi_QRecord.png)

- Следующим добавляем  процессор с UpdateRecord и убираем в поле price `.0` заменяя на 0 значения в поле чтобы привести это поле к числовому формату
![UpdateRecord](./images/nifi_update_pr.png)

- Далее соединяем все файлы в один файл на этапе MergeContent и прописываем настройки объединенния
![MergeContent](./images/nifi_merge.png)

- Объединенные файлы пишем на диск используя PutFile и параллельно отправляем данные в Elasticsearch
![PutFile](./images/nifi_put.png)

### На данном этапе возникла ошибка в ходе выполнения работы, тк merge по какой то непонятной причине не соединяет файлы воедино. Причну найти не получилось.

Общая схема проектам в **Apache Nifi**
![PutFile](./images/NiFi.png)

Ссылка на [xml файл](./bn_templ.xml) с данным схемыApache Nifi